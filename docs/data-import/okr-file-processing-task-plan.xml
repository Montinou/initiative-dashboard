<?xml version="1.0" encoding="UTF-8"?>
<taskPlan version="1.0" stage="Stage-1-File-Upload-and-Processing" scope="ONLY upload + server processing (no bot)">
  <metadata>
    <owner>initiative-dashboard</owner>
    <created>2025-08-11</created>
    <repository>initiative-dashboard</repository>
    <branch>main</branch>
    <references>
      <doc>docs/deployment/environment.md</doc>
      <doc>docs/data-import/okr-file-processing-and-bot-integration.md</doc>
      <doc>.clinerules/*</doc>
      <doc>CLAUDE.md</doc>
    </references>
    <nonGoals>
      <item>No Dialogflow CX bot integration in this plan</item>
      <item>No E2E tests (manual verification only)</item>
    </nonGoals>
  </metadata>

  <principles>
    <item>Apply tasks in baby steps; complete and verify each task before moving to the next</item>
    <item>Follow .clinerules documentation and conventions strictly</item>
    <item>All processing happens on the server; client only uploads via signed URL with metadata</item>
    <item>Respect Supabase RLS; use server client; admin client only on trusted server code paths</item>
    <item>Design APIs and storage layout to be reused seamlessly by Stage 2 (bot integration)</item>
  </principles>

  <prompt><![CDATA[
You are to execute this plan strictly task-by-task using baby steps. At each step:
- Read the .clinerules/* docs first and conform to them.
- Confirm environment variables per docs/deployment/environment.md (GCS + Supabase).
- Make minimal, focused changes. Write/update docs when needed.
- After completion, run lint/build locally and perform a manual verification.
- Do NOT start any task under future stages.
- Ensure all outputs (files, endpoints, migrations) are forward-compatible with Stage 2.
  ]]></prompt>

  <artifacts>
    <api>
      <endpoint method="POST" path="/api/upload/okr-file" purpose="Issue signed URL and required metadata"/>
      <endpoint method="POST" path="/api/upload/okr-file/notify" purpose="Notify and enqueue processing job"/>
      <endpoint method="GET" path="/api/upload/okr-file/jobs/[id]" purpose="Fetch job status"/>
      <endpoint method="GET" path="/api/upload/okr-file/history" purpose="List past uploads (RLS)"/>
      <endpoint method="GET" path="/api/upload/okr-file/stats" purpose="Aggregated stats for uploads"/>
      <endpoint method="GET" path="/api/upload/okr-file/template" purpose="Download OKR template"/>
    </api>
    <storage>
      <bucket>GCS: ${GCS_BUCKET_NAME}</bucket>
      <objectKeyPattern>okr-uploads/{tenant_id}/{yyyy}/{mm}/{dd}/{user_id}/{timestamp}-{checksum}-{sanitized_filename}</objectKeyPattern>
      <metadataKeys>
        <key>tenant_id</key>
        <key>user_id</key>
        <key>area_id</key>
        <key>filename</key>
        <key>checksum</key>
        <key>content_type</key>
        <key>session_id</key>
        <key>source</key>
      </metadataKeys>
    </storage>
    <database>
      <table>okr_import_jobs</table>
      <table>okr_import_job_items</table>
    </database>
    <docs>
      <doc>docs/data-import/okr-file-processing-and-bot-integration.md</doc>
      <doc>docs/deployment/environment.md</doc>
    </docs>
  </artifacts>

  <tasks>
    <task id="T0" title="Baseline and guardrails">
      <objective>Prepare environment, validate credentials, and align with .clinerules before coding</objective>
      <steps>
        <step>Read .clinerules/* and CLAUDE.md for API/auth patterns</step>
        <step>Verify env vars: GCP_PROJECT_ID, GCS_BUCKET_NAME, GCP_SERVICE_ACCOUNT_JSON_BASE64 or GOOGLE_APPLICATION_CREDENTIALS</step>
        <step>Run a minimal GCS auth check script locally (no code changes yet)</step>
      </steps>
      <acceptanceCriteria>
        <item>All required env vars present or documented</item>
        <item>GCS bucket exists and is reachable with service account create a folder for this files</item>
      </acceptanceCriteria>
      <outputs/>
    </task>

    <task id="T1" title="DB migrations: import job tracking (RLS)">
      <objective>Create tables and policies for tracking OKR import jobs and per-row results</objective>
      <steps>
        <step>Create migration files under supabase/migrations for tables okr_import_jobs and okr_import_job_items</step>
        <step>Add RLS policies: tenant-scoped visibility; managers see area-scoped where applicable</step>
        <step>Seed minimal data for local dev if needed (optional)</step>
      </steps>
      <files>
        <file>supabase/migrations/*</file>
      </files>
      <acceptanceCriteria>
        <item>Tables created and RLS enabled</item>
        <item>Policies enforce tenant isolation</item>
      </acceptanceCriteria>
      <dependencies>
        <dep>T0</dep>
      </dependencies>
    </task>

    <task id="T2" title="GCS utility wrapper (signed URLs + metadata)">
      <objective>Provide a server-only utility to generate signed URLs and standardize metadata handling</objective>
      <steps>
        <step>Create utils/gcs.ts with functions: getClient(), generateSignedUploadUrl(), buildObjectKey(), parseObjectMetadata()</step>
        <step>Ensure resumable uploads support as configured (GCS_USE_RESUMABLE_UPLOADS)</step>
        <step>Unit test the key builder and metadata mapping (lightweight)</step>
      </steps>
      <files>
        <file>utils/gcs.ts</file>
      </files>
      <acceptanceCriteria>
        <item>Signed URL generation works locally (manual test)</item>
        <item>Object key uses required pattern and includes checksum</item>
      </acceptanceCriteria>
      <dependencies>
        <dep>T1</dep>
      </dependencies>
    </task>

    <task id="T3" title="API: POST /api/upload/okr-file (issue signed URL)">
      <objective>Implement endpoint that validates auth, builds object path, and returns signed upload URL with required metadata</objective>
      <steps>
        <step>Follow API pattern: server createClient() → auth.getUser() → fetch profile → build tenant/user context</step>
        <step>Validate input: filename, size, checksum (sha256), contentType; enforce MAX_UPLOAD_SIZE_MB and allowlist content types</step>
        <step>Build object key and call GCS util to sign upload</step>
        <step>Return uploadUrl, requiredMetadata, objectPath, maxSizeMB</step>
        <step>Document request/response in docs/data-import/okr-file-processing-and-bot-integration.md</step>
      </steps>
      <files>
        <file>app/api/upload/okr-file/route.ts</file>
      </files>
      <acceptanceCriteria>
        <item>Endpoint returns 200 with signed URL and metadata for an authenticated user</item>
        <item>401/403 on unauthenticated/unauthorized</item>
      </acceptanceCriteria>
      <dependencies>
        <dep>T2</dep>
      </dependencies>
    </task>

    <task id="T4" title="Client upload wiring (minimal UI)">
      <objective>Wire existing OKRFileUpload component to use the new endpoint and upload with metadata</objective>
      <steps>
        <step>Update components/OKRFileUpload.tsx to call POST /api/upload/okr-file</step>
        <step>Perform the PUT/resumable upload directly to GCS including required metadata</step>
        <step>On success, call POST /api/upload/okr-file/notify with objectPath</step>
        <step>Show basic progress and success/error states</step>
      </steps>
      <files>
        <file>components/OKRFileUpload.tsx</file>
      </files>
      <acceptanceCriteria>
        <item>File uploads successfully to GCS; metadata present on the object</item>
        <item>Notify is called with objectPath</item>
      </acceptanceCriteria>
      <dependencies>
        <dep>T3</dep>
      </dependencies>
    </task>

    <task id="T5" title="API: POST /api/upload/okr-file/notify (enqueue processing)">
      <objective>Implement notification endpoint that creates a job entry and schedules processing</objective>
      <steps>
        <step>Validate auth and derive tenant/user/area</step>
        <step>Validate objectPath format and compute idempotency with checksum in key</step>
        <step>Create okr_import_jobs row (status=pending) and return jobId</step>
        <step>Invoke processing asynchronously (inline in dev; prepare hook for later queue/PubSub)</step>
      </steps>
      <files>
        <file>app/api/upload/okr-file/notify/route.ts</file>
      </files>
      <acceptanceCriteria>
        <item>Job created with status pending and returned to client</item>
        <item>Duplicate uploads are detected and linked</item>
      </acceptanceCriteria>
      <dependencies>
        <dep>T4</dep>
      </dependencies>
    </task>

    <task id="T6" title="Processor service (CSV/XLSX parsing, validation, upserts)">
      <objective>Create a server-only processor that reads GCS object, validates rows, and writes results to DB</objective>
      <steps>
        <step>Create service: services/okrImportProcessor.ts with functions: processJob(jobId), parseFile(), validateRow(), mapRowToEntities(), upsert()</step>
        <step>Support CSV and XLSX; include safe parsers and content sniffing</step>
        <step>Write per-row results to okr_import_job_items; update job totals and final status</step>
        <step>Emit dashboard activity events where applicable</step>
      </steps>
      <files>
        <file>services/okrImportProcessor.ts</file>
      </files>
      <acceptanceCriteria>
        <item>Processing succeeds for a small sample file; job transitions to completed with accurate totals</item>
        <item>Validation errors recorded per row with actionable messages</item>
      </acceptanceCriteria>
      <dependencies>
        <dep>T5</dep>
      </dependencies>
    </task>

    <task id="T7" title="Job status and history endpoints">
      <objective>Expose job status, history, and stats for the UI</objective>
      <steps>
        <step>Create GET /api/upload/okr-file/jobs/[id]</step>
        <step>Create GET /api/upload/okr-file/history and GET /api/upload/okr-file/stats</step>
        <step>Ensure RLS and role scoping in queries</step>
      </steps>
      <files>
        <file>app/api/upload/okr-file/jobs/[id]/route.ts</file>
        <file>app/api/upload/okr-file/history/route.ts</file>
        <file>app/api/upload/okr-file/stats/route.ts</file>
      </files>
      <acceptanceCriteria>
        <item>Endpoints return correct data for the current tenant/user</item>
      </acceptanceCriteria>
      <dependencies>
        <dep>T6</dep>
      </dependencies>
    </task>

    <task id="T8" title="Health checks and observability">
      <objective>Add basic health signals and structured logs for processing</objective>
      <steps>
        <step>Extend /api/health to include gcs connectivity and bucket existence checks</step>
        <step>Add correlationId (jobId) to logs</step>
      </steps>
      <files>
        <file>app/api/health/route.ts</file>
      </files>
      <acceptanceCriteria>
        <item>/api/health reports gcs:true when configured and reachable</item>
      </acceptanceCriteria>
      <dependencies>
        <dep>T7</dep>
      </dependencies>
    </task>

    <task id="T9" title="Documentation updates and manual verification">
      <objective>Finalize docs and run a manual end-to-end upload + processing test</objective>
      <steps>
        <step>Update docs/data-import/okr-file-processing-and-bot-integration.md with any API/flow nuances</step>
        <step>Document manual test script and acceptance checklist</step>
        <step>Perform manual run: request signed URL → upload → notify → poll status → check DB</step>
      </steps>
      <files>
        <file>docs/data-import/okr-file-processing-and-bot-integration.md</file>
      </files>
      <acceptanceCriteria>
        <item>Docs reflect the final implementation</item>
        <item>Manual test passes and artifacts (job, items) exist</item>
      </acceptanceCriteria>
      <dependencies>
        <dep>T8</dep>
      </dependencies>
    </task>
  </tasks>

  <futureCompatibilityNotes>
    <item>All endpoints and job schema are designed to be called by future bot endpoints without modification</item>
    <item>Notify endpoint returns jobId to be consumed by bot for status polling</item>
    <item>GCS object metadata includes 'source' so later bot uploads can be distinguished</item>
    <item>Processor service is decoupled and callable from API, queues, or webhooks later</item>
  </futureCompatibilityNotes>

  <risks>
    <risk>
      <name>Large files and serverless timeouts</name>
      <mitigation>Use resumable uploads; keep processing async; design for migration to Pub/Sub or background workers</mitigation>
    </risk>
    <risk>
      <name>RLS misconfiguration</name>
      <mitigation>Add explicit tests/queries verifying tenant filters; follow core API pattern</mitigation>
    </risk>
    <risk>
      <name>Inconsistent schemas vs dashboards</name>
      <mitigation>Map and validate to existing DB shapes used by dashboard; version row mappers</mitigation>
    </risk>
  </risks>
</taskPlan>
